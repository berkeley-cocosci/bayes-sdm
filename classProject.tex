% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified: Niels Taatgen (taatgen@cmu.edu) 10/24/2006

%% Change ``a4paper'' in the following line to ``letterpaper'' if you are
%% producing a letter-format document.

\documentclass[10pt,letterpaper]{article}

% uncomment below to put in cogsci format
%\usepackage{cogsci}

\usepackage{pslatex}
\usepackage{apacite}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{float}
\usepackage{mathrsfs}
\usepackage{fullpage}


\newcommand{\footnoteremember}[2]{
\footnote{#2}
\newcounter{#1}
\setcounter{#1}{\value{footnote}}
}
\newcommand{\footnoterecall}[1]{
\footnotemark[\value{#1}]
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{{\bf VS265 Final Project}\\Exemplar storage in associative memory systems}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% authors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{{\large \bf Joshua T. Abbott (joshua.abbott@berkeley.edu)\footnoteremember{myfootnote}{The authors contributed equally to this work.}} \\
 {\large \bf Jessica B. Hamrick (jhamrick@berkeley.edu)\footnoterecall{myfootnote}} \\
% {\large \bf Thomas L. Griffiths (tom\_griffiths@berkeley.edu)} \\
  Department of Psychology, University of California, Berkeley, CA 94720 USA}

\date{}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
We explore the storage of exemplars in associative memory systems as an initial investigation into neural implementations of psychological processes. We outline a recent proposal that probabilistic models of cognition accounting for human behavior at a computational level of analysis can be approximated by existing psychological process models, namely, \textit{exemplar models}. Given this motivation, the main contribution of this paper consists of analyzing the dynamics and storage capabilities of two associative models of memory: a Hopfield network, and a Sparse Distributed Memory (SDM) system. We describe each model in a common mathematical framework and compare their performance  on storage capacity, retrieving prototypes, and retrieving sequences, using both corrupted and uncorrupted stimuli. Finding SDMs to be more robust and powerful computing machines, we then describe how the mechanics of SDMs can be interpreted probabilistically as a Monte Carlo method called Importance Sampling. We conclude with future directions of this research. 

% \textbf{Keywords:} 
% Sparse Distributed Memory systems, Bayesian inference, Exemplar model, Rational Process Model, Importance Sampling.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% something broad about prob models of cognition 
\cite{griffiths2010probabilistic,tenenbaum2011grow}

% something about issues with prob models
\cite{kahneman1972subjective,gigerenzer2000simple}

% something about marrs levels and that approach
\cite{marr82,anderson90}

% something about addressing these issues with RPM and bridging levels
\cite{sanborn2010rational,griffiths2012bridging}

% in particular we look at exemplar model approx
\cite{Shi2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - exemplar models / IS for Bayes 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Exemplar models and Bayesian inference}

% maybe start with a simple example of exemplar models like Lei does.. 

% what is an exemplar model
% In an \textit{exemplar model} model of memory, people store individual instances of observations in memory (as an \textit{exemplar}) and evaluate new observations by activating these stored exemplars as a function of their similarity to this novel event \cite{medin1978context,nosofsky1986attention}. More formally, let $X^{*} = \{ x^{*}_{1}, x^{*}_{2}, \ldots, x^{*}_{n} \}$ represent a set of $n$ stored exemplars, and define $s(x,x^{*})$ to be a similarity function measuring the distance between an observation $x$ and a stored exemplar $x^{*}$. 

% -- i dont know if i like the above, i'm too tired..

% something about an identification task

\begin{equation}
	p_{r}(x^{*}_{i}|x)=\frac{s(x,x^{*}_{i})}{\sum^{n}_{j=1}s(x,x^{*}_{j})}
\end{equation}

% something about exemp models can do prob. categorization
\cite{ashby1995categorization}

% however a more general form of exemp models is

\begin{equation}
	\hat{f}(x)=\frac{\sum^{n}_{j=1}f_{j}\,s(x,x^{*}_{j})}{\sum^{n}_{j=1}s(x,x^{*}_{j})}
\end{equation}

% very brief bayes

% very brief intro to importance sampling

\cite{neal1993probabilistic,Shi2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - previous implementations: RBFs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neural implementations of Importance Sampling}
\cite{Shi2009}

% review Lei's components of an importance sampler

% math of RBF

% *** interesting because the RBF network is not cognitively satisfying \\
% **** all grandmother cells (and the proposed spiking neuron model was using an RBF too..) \\
% **** not robust \\


% what is an associative memory model

% something about distributed memory

% the rest of the paper explores the role of exemplars and associative memory


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% associative models of memory
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Associative models of memory}

Associative models of memory comprise a set of information processing systems with the aim of storing and retrieving data through finding associations in similar patterns of data. We consider two models in particular: a Hopfield network, and a Sparse Distributed Memory (SDM) system. We focus on these two models since Hopfield networks were covered in class; providing a foundation for which to compare to, and the mathematics between these two models has been previously presented in a unified framework \cite{Keeler1988}; allowing us to better understand the differences between each.  We present each model in a similar framework below. For a more detailed exploration between these particular associative memory models, refer to \citeA{Keeler1988}, and for an alternative to SDMs that we do not explore (incorporating Hamming distance on binary patterns), please see \citeA{Lippmann1987}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - Hopfield networks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hopfield networks}

Hopfield networks are an autoassociative model of memory, where a set of binary (+1/-1) patterns can be stored in the weights of a neural network and a pattern can be recovered from a noisy stimulus \cite{Hopfield1982}. More formally, given a set of binary patterns $\bf{p}^{\alpha}$ of length $N$, a symmetric $N \times N$ matrix $\bf{T}$ stores these patterns using the outer-product rule: $\bf{T} = \bf{T} + \bf{p}^{\alpha}\,\bf{p}^{\alpha\,'}$. To recover one of these stored patterns, a noisy input $\bf{x}$ is supplied to the network and a stochastic form of Hebbian learning is applied. For each  iteration $t$, a random output node $\bf{z}_{j}$ is selected and updated to (+1/-1) depending on the sign of $\sum_{j=1}^{N} \bf{T}_{ij}\,\bf{z}_{j}$. The Hopfield network architecture is depicted in panel (a) of Figure \ref{neuralNets} below.

While the Hopfield network has received popularity for its simplicity as a neural model of memory, a number of limitations have been posited in the literature \cite{Keeler1988}: (1) The storage capacity of a Hopfield network is bounded by a small fraction of the number of processing units - i.e., the length of the input; (2) they cannot handle temporal sequences of patterns; (3) the requirement of symmetric weights in $\bf{T}$ make Hopfield networks  implausible as a model of the brain; and (4) Hopfield networks have limited abilities to store correlated patterns. We explore a number of these concerns in Section 3, our analysis of exemplar storage in Hopfield networks and SDMs.

\begin{center}
\begin{figure*}[h!]
{
	\hfill{}
	\begin{tabular}{lclclc}
	\raisebox{1.6in}{(a)} &
		\includegraphics[width=0.23\textwidth]{./figures/hopfieldNetwork.png} &
	\raisebox{1.6in}{(b)} &
		\includegraphics[width=0.27\textwidth]{./figures/sdmNetwork.png} 
	\raisebox{1.6in}{(c)} &
		\includegraphics[width=0.29\textwidth]{./figures/sdmOperations.png} &
	\end{tabular}
}
\hfill{}
\caption{(a) Neural network implementation of a Hopfield network. (b) Neural network implementation of an SDM. Both networks take binary inputs $\bf{x}_{i}$ of length $N$ and produce binary outputs $\bf{z}_{i}$ of length $N$. The Hopfield network uses a set of symmetric weights $\bf{T}_{ij}$ and a Hebbian learning rule to recover an exemplar from a noisy input. The weights between the input and hidden layer in a SDM network determine the set of addresses $\bf{a}_{m}$ to read from or write to, and the second layer of weights store the contents of memory at those addresses. (c) An illustration of the basic read/write operations over SDMs. The outer dotted line represents the space of $2^{N}$ possible addresses while the squares with labels $\bf{A}_{m}$ represent the $M$ sampled hard addresses used for storage. The address being requested for operation is the $\bf{x}$ in the center of the blue circle of radius $H$. The hard addresses selected for operating correspond to the blue squares within the Hamming radius of $\bf{x}$.}
\label{neuralNets}
\end{figure*}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - SDM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sparse Distributed Memory (SDM) systems}

Sparse Distributed Memory (SDM) systems were developed by \citeA{Kanerva1988,Kanerva1993} as a mathematical model of human memory, exploiting the properties of high-dimensional representational spaces.
An SDM can function as an \textit{autoassociative}, \textit{content-addressable} memory (associates a pattern with itself) like a Hopfield network, but can additionally function as a \textit{heteroassociative} memory (associates one pattern with another), and as a \textit{sequential-access} memory (in which a temporal sequence of patterns can be stored and retrieved). A simple way to understand how SDMs work follows from an analogy to conventional computer memory. In typical Random-Access Memory (RAM), bit-words of length $N$ are stored in an array of $M = 2^{N}$ registers. To read from or write data to memory, all that is needed is the data itself (the \textit{contents} of memory), and the location that points to this content (the \textit{address}). This limits the amount of storage in memory (since $M$ is fixed based on the length a word) and the contents in memory have nothing to do with their address in memory (thus one cannot determine the similarity of memory content based their addresses). SDMs were developed to overcome this limitation and to encapsulate the notion that distances between concepts in memory correspond to distances between points in high-dimensional space \cite{Kanerva1988}. Instead of storing and reading data in a sequentially-enumerated set of $M = 2^{N}$ address registers, an SDM considers very bit-words of length $N$ (typically $N > 100$) and samples a very large number $M >> N$ set of hard addresses to use as registers from the space of $2^{N}$ possible addresses. Each point in this sampled space of addresses will be far away from most other points in the sample space (as measured by Hamming distance).

To store a set of binary patterns $\bf{p}^{\alpha}$ of length $N$ in an autoassociative SDM, an address $\bf{a}$ and data $\bf{d}$ must be supplied for each pattern corresponding to a tuple ($\bf{x}^{\alpha}$, $\bf{w}^{\alpha}$), where $\bf{x}^{\alpha}$ = $\bf{w}^{\alpha}$. When acting as a heteroassociative or sequential-access memory system, this equality need not be held. Since the set of $M$ hard addresses does not enumerate the total space of $2^{N}$, a particular address $\bf{x}^{\alpha}$ has high probability of not existing in the sampled addresses space. Thus, existing hard addresses near $\bf{x}^{\alpha}$ are selected for access if they are within a Hamming radius $H$ of $\bf{x}^{\alpha}$, where $H$ represents the number of different bits allowed between points. The contents of these selected addresses are then modified to store the pattern $\bf{w}^{\alpha}$ such that each bit in the contents is increased or decreased by 1 depending on whether or not that bit in the pattern is a 1 or 0, respectively. To read a binary pattern $\bf{z}$ out of memory from address $\bf{x}$, a similar selection procedure is carried out in which a set of hard addresses within a Hamming radius of $\bf{x}$ are accessed, and a bit $j$ in the output $\bf{z}$ is set to 1 or 0 if the sum of the contents in position $j$ over these selected addresses is greater than or less 0, respectively. A neural network implementation of an SDM and an illustrative schematic of an SDM's operation are provided in panels (b) and (c) of Figure \ref{neuralNets} above.

More formally, given a set of binary patterns $\bf{p}^{\alpha}$ of length $N$ in the form ($\bf{x}^{\alpha}$, $\bf{w}^{\alpha}$), an SDM can be constructed as a neural network with $N$ units in the input layer, a hidden layer with $M$ units for each sampled hard addresses, and an output layer with $N$ units. The weights between the input and hidden layer correspond to the $M \times N$ matrix $\bf{A}$ of hard-addresses, and the weights between the hidden and output layer correspond to the $M \times N$ matrix $\bf{C}$ of contents stored at each address. The rule for writing $\bf{w}^{\alpha}$ to memory address $\bf{x}^{\alpha}$ is expressed as:
\begin{align}
 \bf{y}^{\alpha} &= \Theta_{H}(\bf{A}\,\bf{x}^{\alpha}) \\	
 \bf{C} &= \bf{w}^{\alpha}\,\bf{y}^{\alpha} 
\end{align}

\noindent where $\bf{y}^{\alpha}$ is a binary vector of length $M$ with a 1 in position $j$ if hard address $\bf{a}$ is within Hamming distance $H$ of $\bf{x}^{\alpha}$. The rule for reading $\bf{z}$ from memory address $\bf{x}$ is expressed as:
\begin{align}
 \bf{y} &= \Theta_{H}(\bf{A}\,\bf{x}) \\	
 \bf{z} &= g(\bf{C}\,\bf{y}) 
\end{align}
\noindent where $g(x_{i}) = 1$ if $x_{i} > 0$ and $0$ if $x_{i} \leq 0$.  




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% model evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exemplar storage in associative memory systems}



We compared the Hopfield and SDM models using several different
metrics to gauge their fitness as exemplar storage systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - capacity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Storage Capacity}

First, we explored the upper bound of storage capacity of the SDM
versus the Hopfield network by generating $k$ random, uncorrelated
inputs, storing them in and then retriving them from the different
models, and then observing the amount of corruption introduced during
this procedure. We tested several SDMs with different address space
sizes, $M\in\{500, 1000, 2500, 5000, 10000\}$ (the Hopfield network
only has one address space size, i.e. $M=N$).  Figure
\ref{fig:capacity}a shows the results of this comparison using
uncorrupted inputs for $k\leq 200$ (number of exemplars). For
sufficiently large $M$, the SDM easily outperforms the Hopfield
network.

The SDM and Hopfield network should be able to work with corrupted
inputs as well. So, we performed the same analysis as above, except
that we attempted to retrieve each exemplar with a corrupted version
of itself and compared the SDM with $M=10000$.  \ref{fig:capacity}b
shows these results for different levels of corruption. At low levels
of corruption, the SDM is more resistant to corrupted inputs than the
Hopfield net. 

Interestingly, at higher levels of noise, the SDM is significantly
more error-prone than the Hopfield network. We suspect this is due to


\begin{center}
\begin{figure}[t!]
{
	\hfill{}
	\begin{tabular}{lclc}
	\raisebox{1.75in}{(a)} &
		\includegraphics[width=0.4\textwidth]{./figures/capacity-edit.png}
	\raisebox{1.75in}{(b)} &
		\includegraphics[width=0.4\textwidth]{./figures/tolerance-edit.png} 
	\end{tabular}
}
\hfill{}
\caption{SDM and Hopfield network capacities (panel a) and error tolerances (panel b).}
\label{fig:capacity}
\end{figure}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - prototypes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Prototype Retrieval}

% what happens when we try to store the noisy circles in an hopnet, does it converge to a prototype as well?


\begin{center}
\begin{figure*}[t!]
{
	\hfill{}
	\begin{tabular}{ l l l c}
	\raisebox{2.5in}{(a)} &
		\includegraphics[width=0.43\textwidth]{./figures/exemplars.png} &
	\raisebox{2.5in}{(b)} &
		\includegraphics[width=0.4\textwidth]{./figures/prototypeResults.png} \\
	%	\includegraphics[width=0.4\textwidth]{./figures/vi_sdm.png} 
	\end{tabular}
}
\hfill{}
\caption{(a) Nine noisy exemplars stored in both SDM and Hopfield models. (b) The retrieval results for the Hopfield network (top) and SDM network (bottom) when queried with a noisy word.}
\label{prototypes}
\end{figure*}
\end{center}


% - even if it does, it can't really store much else without really messing things up due to its capacity, where in SDM it's just a tiny fraction of the space of things - ie, does storing noisy faces, numbers, circles, AND sequences do anything to the retrieval of the prototype?

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.95\textwidth]{./figures/prototype-edit.png}

\end{center}
\caption{This is a figure.} 
\label{sample-figure}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - sequences
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sequence Storage}

% just to show off here since the hopnet can't at all - but maybe we should make a comment about recurrent nets (JUST a comment) ?
\begin{center}
\begin{figure*}[t!]
{
	\hfill{}
	\begin{tabular}{ l c }
	\raisebox{4.1in}{(a)} &
		\includegraphics[width=0.8\textwidth]{./figures/exemplarStoredSequences.png} \vspace{25bp} 
		\\
	\raisebox{.6in}{(b)} &
		\includegraphics[width=0.8\textwidth]{./figures/prototypeRetrievedSequence.png} 
	\end{tabular}
}
\hfill{}
\caption{(a) Exemplars of noisy sequences stored in SDM memory. (b) Retrieval of prototypical sequence in SDM from a noisy word.}
\label{sequences}
\end{figure*}
\end{center}



%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection - discussion
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}

% wrap up results from evaluations

% ** SDM is the best \\
% only the only model that is both robust to noise/storage capacity and
% sophisticated enough to behave in various ways 

** difference between addresses and data \\
% - address/data difference is called heteroassociative memory (associate one pattern with another - i think...)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% future directions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic interpretations of SDMs}

% *** probabilistic interpretations \cite{Anderson1989}\\
% *** figure out the encoding for importance sampling \\
% *** how close does it have to be to the delta function? \\

Exemplar models have more potential than uniformly storing data and
recovering its mean. \citeA{Shi2010} demonstrated that exemplar models
can be used to implement \textit{importance sampling}, a Monte Carlo
method of approximating Bayesian inference.  Can SDMs be used to
implement importance sampling? Previous work has formalized a
probabilistic interpretation of SDMs \cite{Anderson1989}, thus
suggesting that it should be possible to adjust that formalization to
specifically encompass exemplar-based approximate inference. Here, we
extend the idea of using exemplar models for importance sampling in
the context of SDMs.

\subsection{Overview}

In general, our goal is to use SDMs to recover the expectation of some
function $f(x^*)$. However, we cannot directly observe $x^*$; instead,
we have observations $x$. Importance sampling provides a way for us to
infer the value of $x^*$ given our observations, and therefore compute
$y$. This is given by Eq. 12 in \citeA{Shi2010}:
\begin{equation}
E[f(x^*)|x]\approx \frac{\sum_j f(x_j^*)\Pr(x|x_j^*)}{\sum_j \Pr(x|x_j^*)}
\end{equation}

To translate this into the context of a SDM, let $x^*$ be the
uncorrupted target address at which we want to read or write a value
$y$. If $x$ is a corrupted version of $x^*$, then we wish to choose
some addresses $a_j$ given $x$ such that reading and writing to those
addresses approximate reading and writing $(x^*,y)$. This is
equivalent to computing the expectation of $f(x^*)=y$ over the
addresses $a_j$.

\subsection{Writing}

Let us define the probability of writing to address $a_j$ given an
input address $x_i^*$ as $w(a_j|x_i^*)$. In the limit, the number of
addresses increases to the point where we will always be able to write
to exactly $x_i^*$. Thus, this writing density must satisfy the
following constraint:
\begin{equation}
\lim_{M\rightarrow 2^N}w(a_j|x_i^*) = \delta(a_j=x_i^*)
\label{eq:w}
\end{equation}

After writing multiple values $(x_i^*, y)$, the value of the counter
associated with address $a_j$ will be:
\begin{equation}
C_j=\sum_i w(a_j|x_i^*)y_i
\label{eq:Cj}
\end{equation}

\subsection{Reading}

We are given an address $x$, which as before is a corrupted version of
$x^*$; the goal is to read the value stored at $x^*$. We can define
reading distribution as the likelihood that $x$ was generated from
some $a_j$:
\begin{equation}
r(x| a_j)\propto \Pr(x|x^*=a_j)
\label{eq:r}
\end{equation}

We weight the counter value $C_j$ for each $a_j$ based on the
likelihood that $a_j$ is the actual target address. The sum over these
values is:
\begin{equation}
f(x)=\sum_j C_j r(x| a_j)
\label{eq:R}
\end{equation}

% \begin{equation}
% R(x) = 
% \end{equation}

The expected value of this sum over all addresses $a$ is obtained by
substituting Eq. \ref{eq:Cj} into Eq. \ref{eq:R} and simplifying due
to linearity of expectation:
\begin{align}
\mathbb{E}_a[f(x)]&=\mathbb{E}_a\left[\sum_j\left(\sum_i w(a_j| x_i^*)y_i\right)r(x|a_j)\right]\\
&=\sum_i\ y_i\cdot E_a\left(\sum_j w(a_j| x_i^*)r(x|a_j)\right)
\end{align}

As our address space grows larger (as in Eq. \ref{eq:w}), this approaches:
\begin{align}
\lim_{M\rightarrow 2^N} \mathbb{E}_a[f(x)]&=\sum_i\ y_i \int_a \delta(a-x_i^*)\Pr(x|x^*=a)\ \mathrm{d}a\\
&= \sum_i\ y_i\Pr(x|x_i*)
\end{align}

Thus, in the limit, the expected value that we read out will be:
\begin{equation}
\mathbb{E}[y|x]\propto \sum_i\ y_i\Pr(x|x_i^*)
\end{equation}

\subsection{Empirical comparison}

To verify this formulation of the SDM as an importance sampler, we 
\\
Additionally, since SDMs can function as a \textit{heteroassociative} memory, where \textit{address} and \textit{content} do not need to be the same ..

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  conclusions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}



% \begin{table}[!ht]
% \begin{center} 
% \caption{Sample table title.} 
% \label{sample-table} 
% \vskip 0.12in
% \begin{tabular}{ll} 
% \hline
% Error type    &  Example \\
% \hline
% Take smaller        &   63 - 44 = 21 \\
% Always borrow~~~~   &   96 - 42 = 34 \\
% 0 - N = N           &   70 - 47 = 37 \\
% 0 - N = 0           &   70 - 47 = 30 \\
% \hline
% \end{tabular} 
% \end{center} 
% \end{table}

% \begin{figure}[ht]
% \begin{center}
% \fbox{CoGNiTiVe ScIeNcE}
% \end{center}
% \caption{This is a figure.} 
% \label{sample-figure}
% \end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{classProject}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% appendix (code)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Project Code}
This is where we have the code.


\end{document}
